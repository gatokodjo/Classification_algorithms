{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d89e08b7",
   "metadata": {},
   "source": [
    "# Feature Engineering & Selection — Corrected Notebook with Explanations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80f817b",
   "metadata": {},
   "source": [
    "This notebook demonstrates **feature engineering and selection** on the Titanic dataset.  \n",
    "We will cover:  \n",
    "1. Data cleaning & imputation  \n",
    "2. Encoding categorical variables  \n",
    "3. Feature scaling  \n",
    "4. Train/test split  \n",
    "5. Filter, Wrapper, and Embedded feature selection methods  \n",
    "6. Final pipeline with preprocessing + feature selection + model  \n",
    "Each code cell is preceded by explanation in markdown."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf904446",
   "metadata": {},
   "source": [
    "## Cell 1 — Imports and dataset load\n",
    "We import libraries and load the Titanic dataset from seaborn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd057bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import SelectKBest, RFE, mutual_info_classif\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Load Titanic dataset\n",
    "df = sns.load_dataset(\"titanic\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5042aeb3",
   "metadata": {},
   "source": [
    "## Cell 2 — Data cleaning & preprocessing\n",
    "We inspect missing values, impute them, and drop redundant columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe228d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Missing values before:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Imputers\n",
    "imputer_num = SimpleImputer(strategy=\"median\")\n",
    "imputer_cat = SimpleImputer(strategy=\"most_frequent\")\n",
    "\n",
    "df[[\"age\"]] = imputer_num.fit_transform(df[[\"age\"]])\n",
    "df[[\"embarked\"]] = imputer_cat.fit_transform(df[[\"embarked\"]])\n",
    "\n",
    "# Drop irrelevant/leaky columns\n",
    "cols_to_drop = [\"deck\", \"embark_town\", \"alive\", \"class\", \"who\", \"adult_male\"]\n",
    "df = df.drop(columns=[c for c in cols_to_drop if c in df.columns])\n",
    "\n",
    "print(\"\\nMissing values after imputation:\")\n",
    "print(df.isnull().sum())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650bbce1",
   "metadata": {},
   "source": [
    "## Cell 3 — Encoding categorical variables\n",
    "We encode categorical features into numeric codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7432484a",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = df.select_dtypes(include=['object','category']).columns.tolist()\n",
    "print(\"Categorical columns:\", categorical_cols)\n",
    "\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col].astype(str))\n",
    "\n",
    "print(\"Remaining object columns:\", df.select_dtypes(include=['object']).columns.tolist())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23923f0",
   "metadata": {},
   "source": [
    "## Cell 4 — Feature scaling\n",
    "We standardize numeric features and keep a MinMax copy for chi2 if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2428e063",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = ['age','fare']\n",
    "\n",
    "scaler_std = StandardScaler()\n",
    "scaler_mm = MinMaxScaler()\n",
    "\n",
    "df[num_cols] = scaler_std.fit_transform(df[num_cols])\n",
    "df_mm = df.copy()\n",
    "df_mm[num_cols] = scaler_mm.fit_transform(df_mm[num_cols])\n",
    "\n",
    "df[num_cols].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5d1ac4",
   "metadata": {},
   "source": [
    "## Cell 5 — Train/test split\n",
    "We separate features (X) and target (y), then split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833d1b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=[\"survived\"]) if \"survived\" in df.columns else df.copy()\n",
    "y = df[\"survived\"]\n",
    "\n",
    "non_numeric = X.select_dtypes(include=['object']).columns.tolist()\n",
    "print(\"Non-numeric columns:\", non_numeric)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a4e2bc",
   "metadata": {},
   "source": [
    "## Cell 6 — Filter method (Mutual Information)\n",
    "We use `SelectKBest(mutual_info_classif)` to rank features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf093e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = SelectKBest(score_func=mutual_info_classif, k=5)\n",
    "selector.fit(X_train, y_train)\n",
    "\n",
    "selected_features = X_train.columns[selector.get_support()]\n",
    "print(\"Top 5 features (Mutual Info):\", list(selected_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcfee14d",
   "metadata": {},
   "source": [
    "## Cell 7 — Wrapper method (RFE)\n",
    "Recursive Feature Elimination using Logistic Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8819470",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = LogisticRegression(max_iter=1000)\n",
    "rfe = RFE(estimator, n_features_to_select=5)\n",
    "rfe.fit(X_train, y_train)\n",
    "\n",
    "rfe_features = X_train.columns[rfe.get_support()]\n",
    "print(\"Top 5 features (RFE):\", list(rfe_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed20e7aa",
   "metadata": {},
   "source": [
    "## Cell 8 — Embedded method (Random Forest)\n",
    "We fit a Random Forest and inspect feature importances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e76a1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "importances = pd.Series(rf.feature_importances_, index=X_train.columns)\n",
    "importances.sort_values().plot(kind=\"barh\", figsize=(8,6))\n",
    "plt.title(\"Feature Importances (Random Forest)\")\n",
    "plt.show()\n",
    "\n",
    "importances.sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd5e9c2",
   "metadata": {},
   "source": [
    "## Cell 9 — Full Pipeline (Best Practice)\n",
    "We combine preprocessing, feature selection, and model into one pipeline to avoid data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43641c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = ['age','fare','sibsp','parch']\n",
    "cat_cols = ['sex','embarked']\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', num_pipeline, num_cols),\n",
    "    ('cat', cat_pipeline, cat_cols)\n",
    "])\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('prep', preprocessor),\n",
    "    ('select', SelectKBest(mutual_info_classif, k=8)),\n",
    "    ('clf', RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "scores = cross_val_score(pipe, df.drop(columns=['survived']), df['survived'], cv=5, scoring='roc_auc')\n",
    "print(\"CV ROC-AUC mean:\", scores.mean(), \"std:\", scores.std())"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
